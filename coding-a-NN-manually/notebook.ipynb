{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "colab": {
      "name": "analysis.ipynb",
      "provenance": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "UrQwTakk8CfB"
      },
      "source": [
        "# Reading the dataset\n",
        "import pandas as pd\n",
        "from math import exp\n",
        "from random import seed\n",
        "from random import random\n",
        "\n",
        "df = pd.read_csv(\"https://raw.githubusercontent.com/jbrownlee/Datasets/master/wheat-seeds.csv\", header = None)"
      ],
      "id": "UrQwTakk8CfB",
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "1buEyYGLggKk",
        "outputId": "306c6119-4532-4a8f-93a6-64f40f15e5e4"
      },
      "source": [
        "df.head()"
      ],
      "id": "1buEyYGLggKk",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>15.26</td>\n",
              "      <td>14.84</td>\n",
              "      <td>0.8710</td>\n",
              "      <td>5.763</td>\n",
              "      <td>3.312</td>\n",
              "      <td>2.221</td>\n",
              "      <td>5.220</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>14.88</td>\n",
              "      <td>14.57</td>\n",
              "      <td>0.8811</td>\n",
              "      <td>5.554</td>\n",
              "      <td>3.333</td>\n",
              "      <td>1.018</td>\n",
              "      <td>4.956</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>14.29</td>\n",
              "      <td>14.09</td>\n",
              "      <td>0.9050</td>\n",
              "      <td>5.291</td>\n",
              "      <td>3.337</td>\n",
              "      <td>2.699</td>\n",
              "      <td>4.825</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>13.84</td>\n",
              "      <td>13.94</td>\n",
              "      <td>0.8955</td>\n",
              "      <td>5.324</td>\n",
              "      <td>3.379</td>\n",
              "      <td>2.259</td>\n",
              "      <td>4.805</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>16.14</td>\n",
              "      <td>14.99</td>\n",
              "      <td>0.9034</td>\n",
              "      <td>5.658</td>\n",
              "      <td>3.562</td>\n",
              "      <td>1.355</td>\n",
              "      <td>5.175</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       0      1       2      3      4      5      6  7\n",
              "0  15.26  14.84  0.8710  5.763  3.312  2.221  5.220  1\n",
              "1  14.88  14.57  0.8811  5.554  3.333  1.018  4.956  1\n",
              "2  14.29  14.09  0.9050  5.291  3.337  2.699  4.825  1\n",
              "3  13.84  13.94  0.8955  5.324  3.379  2.259  4.805  1\n",
              "4  16.14  14.99  0.9034  5.658  3.562  1.355  5.175  1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9pViRH88_PaG"
      },
      "source": [
        "# Initialize Network\n",
        "\n",
        "A network is organized into layers. The input layer is really just a row from our training dataset. The first real layer is the hidden layer. This is followed by the output layer that has one neuron for each class value.\n",
        "\n",
        "So initializing weights for each beta coefficient i.e. numberof inputs+1 in each neuron of the hidden layer. Now this output of hidden layer will be sent to the output layer which will have n number of neurons where n is the number of classes in the dataset. Each output neuron will have 2 coefficients one for the ouput from the hidden layer and other as the intercept.\n",
        "\n",
        "Basically weights to be initialized for the current neuron are neurons in the previous layer + 1"
      ],
      "id": "9pViRH88_PaG"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N9cq4hsqm7Mx"
      },
      "source": [
        "def initialize_network(n_inputs, n_hidden, n_outputs):\n",
        "\tnetwork = list()\n",
        "\thidden_layer = [{'weights':[random() for i in range(n_inputs + 1)]} for i in range(n_hidden)]\n",
        "\tnetwork.append(hidden_layer)\n",
        "\toutput_layer = [{'weights':[random() for i in range(n_hidden + 1)]} for i in range(n_outputs)]\n",
        "\tnetwork.append(output_layer)\n",
        "\treturn network"
      ],
      "id": "N9cq4hsqm7Mx",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TGwtBcnAhAtU",
        "outputId": "7fff9aca-744b-465b-83be-1e4bfae68014"
      },
      "source": [
        "seed(1)\n",
        "network = initialize_network(2, 2, 2)\n",
        "for layer in network:\n",
        "\tprint(layer)"
      ],
      "id": "TGwtBcnAhAtU",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[{'weights': [0.13436424411240122, 0.8474337369372327, 0.763774618976614]}, {'weights': [0.2550690257394217, 0.49543508709194095, 0.4494910647887381]}]\n",
            "[{'weights': [0.651592972722763, 0.7887233511355132, 0.0938595867742349]}, {'weights': [0.02834747652200631, 0.8357651039198697, 0.43276706790505337]}]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BmL_Rpq_sAlO"
      },
      "source": [
        "# Forward Propogation\n",
        "We can calculate an output from a neural network by propagating an input signal through each layer until the output layer outputs its values.\n",
        "It has 3 parts to it:-\n",
        "1. Neuron Activation\n",
        "2. Neuron Transfer\n",
        "3. Forward Propagation\n",
        "\n",
        "Storeplanning NN logic"
      ],
      "id": "BmL_Rpq_sAlO"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N9_XTsSeuvRd"
      },
      "source": [
        "## Neuron Activation\n",
        "It is the z value of the neuron or the weighted sum of outputs from all the neurons in the previous layers"
      ],
      "id": "N9_XTsSeuvRd"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fCn5VgJMyYOn"
      },
      "source": [
        "# Calculate neuron activation for an input\n",
        "def activate(weights, inputs):\n",
        "\tactivation = weights[-1]\n",
        "\tfor i in range(len(weights)-1):\n",
        "\t\tactivation += weights[i] * inputs[i]\n",
        "\treturn activation"
      ],
      "id": "fCn5VgJMyYOn",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b7_ODfY_xDZZ"
      },
      "source": [
        "## Neuron Transfer\n",
        "We need to transfer this activation through a non linear function to see what the output actually is. We use a transfer function for this, in this case we use sigmoid function."
      ],
      "id": "b7_ODfY_xDZZ"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "87JCXQk0ybE6"
      },
      "source": [
        "# Transfer neuron activation\n",
        "def transfer(activation):\n",
        "\treturn 1.0 / (1.0 + exp(-activation))"
      ],
      "id": "87JCXQk0ybE6",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TrntZuRay_3E"
      },
      "source": [
        "## Forward Propagation\n",
        "We work through each layer of our network calculating the outputs for each neuron. All of the outputs from one layer become inputs to the neurons on the next layer."
      ],
      "id": "TrntZuRay_3E"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2zmSpiXhzXbt"
      },
      "source": [
        "# Forward propagate input to a network output\n",
        "def forward_propagate(network, row):\n",
        "\tinputs = row\n",
        "\tfor layer in network:\n",
        "\t\tnew_inputs = []\n",
        "\t\tfor neuron in layer:\n",
        "\t\t\tactivation = activate(neuron['weights'], inputs)\n",
        "\t\t\tneuron['output'] = transfer(activation)\n",
        "\t\t\tnew_inputs.append(neuron['output'])\n",
        "\t\tinputs = new_inputs\n",
        "\treturn inputs"
      ],
      "id": "2zmSpiXhzXbt",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_fxVPVOBzYeh",
        "outputId": "01444694-393c-4dd9-c00d-dd1b0bfe184a"
      },
      "source": [
        "# Testing out the forward propogation\n",
        "network = [[{'weights': [0.13436424411240122, 0.8474337369372327, 0.763774618976614]}],\n",
        "\t\t[{'weights': [0.2550690257394217, 0.49543508709194095]}, {'weights': [0.4494910647887381, 0.651592972722763]}]]\n",
        "row = [1, 0]\n",
        "output = forward_propagate(network, row)\n",
        "print(output)"
      ],
      "id": "_fxVPVOBzYeh",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.6629970129852887, 0.7253160725279748]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xFmYRmMN0pJv"
      },
      "source": [
        "# Back Propagate Error"
      ],
      "id": "xFmYRmMN0pJv"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZIchpLYWqmy0"
      },
      "source": [
        "The weights of the neural network are trained when the error from the forward propagation is sent back to each of the weights of the neuron and these weights are manipulated to minimize this error.\n",
        "\n",
        "The backpropation can be split into 2 steps:-\n",
        "1. Transfer Derivative\n",
        "2. Error Backpropagation\n",
        "\n",
        "## Transfer Derivative\n",
        "Given an output value from a neuron, we need to calculate it’s slope.\n",
        "\n",
        "We are using the sigmoid transfer function, the derivative of which can be calculated as follows:"
      ],
      "id": "ZIchpLYWqmy0"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d79u0hJ9g_wF"
      },
      "source": [
        "# Calculate the derivative of an neuron output\n",
        "def transfer_derivative(output):\n",
        "\treturn output * (1.0 - output)"
      ],
      "id": "d79u0hJ9g_wF",
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6EJE64JvSE81"
      },
      "source": [
        " ## Error Backpropagation\n",
        "First step is to calculate the error for each output neuron, which we can use to navigate through all the neurons in the hidden layers.\n",
        "This is explained by the below function where delta is calculated in the output layer by the formula.\n",
        "\n",
        "error = (weight_k * error_j) * transfer_derivative(output)\n",
        "\n",
        "and in the layer before this takes the weighted sum of all the neurons in the layer after to calculate their delta."
      ],
      "id": "6EJE64JvSE81"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XDBL85-TP4i9"
      },
      "source": [
        "# Backpropagate error and store in neurons\n",
        "def backward_propagate_error(network, expected):\n",
        "\tfor i in reversed(range(len(network))):\n",
        "\t\tlayer = network[i]\n",
        "\t\terrors = list()\n",
        "\t\tif i != len(network)-1:\n",
        "\t\t\tfor j in range(len(layer)):\n",
        "\t\t\t\terror = 0.0\n",
        "\t\t\t\tfor neuron in network[i + 1]:\n",
        "\t\t\t\t\terror += (neuron['weights'][j] * neuron['delta'])\n",
        "\t\t\t\terrors.append(error)\n",
        "\t\telse:\n",
        "\t\t\tfor j in range(len(layer)):\n",
        "\t\t\t\tneuron = layer[j]\n",
        "\t\t\t\terrors.append(expected[j] - neuron['output'])\n",
        "\t\tfor j in range(len(layer)):\n",
        "\t\t\tneuron = layer[j]\n",
        "\t\t\tneuron['delta'] = errors[j] * transfer_derivative(neuron['output'])"
      ],
      "id": "XDBL85-TP4i9",
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qSHDi8S_P6Ac",
        "outputId": "c211b599-9574-436c-aa55-b2bce714c4ae",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# test backpropagation of error\n",
        "network = [[{'output': 0.7105668883115941, 'weights': [0.13436424411240122, 0.8474337369372327, 0.763774618976614]}],\n",
        "\t\t[{'output': 0.6213859615555266, 'weights': [0.2550690257394217, 0.49543508709194095]}, {'output': 0.6573693455986976, 'weights': [0.4494910647887381, 0.651592972722763]}]]\n",
        "expected = [0, 1]\n",
        "backward_propagate_error(network, expected)\n",
        "for layer in network:\n",
        "\tprint(layer)"
      ],
      "id": "qSHDi8S_P6Ac",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[{'output': 0.7105668883115941, 'weights': [0.13436424411240122, 0.8474337369372327, 0.763774618976614], 'delta': -0.0005348048046610517}]\n",
            "[{'output': 0.6213859615555266, 'weights': [0.2550690257394217, 0.49543508709194095], 'delta': -0.14619064683582808}, {'output': 0.6573693455986976, 'weights': [0.4494910647887381, 0.651592972722763], 'delta': 0.0771723774346327}]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V99lLjhYS7UP"
      },
      "source": [
        "# Train Network\n",
        "\n",
        "The network is trained using stochastic gradient descent.\n",
        "\n",
        "This involves multiple iterations of exposing a training dataset to the network and for each row of data forward propagating the inputs, backpropagating the error and updating the network weights.\n",
        "\n",
        "This part is broken down into two sections:\n",
        "1. Update Weights\n",
        "2. Train Network"
      ],
      "id": "V99lLjhYS7UP"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3PvumAAGTLHi"
      },
      "source": [
        "## Update Weights\n",
        "After the calculation of delta we need to update weights of each neuron to reduce the cost function.\n",
        "\n",
        "weight_new = weight_old + learning_rate_fixed * error(delta) * input(value that caused that error)\n",
        "\n",
        "Same thing for bias (b) but there's no input for it.\n",
        "\n",
        "Learning rate controls how much to change the weight to correct for the error. For example, a value of 0.1 will update the weight 10% of the amount that it possibly could be updated. Small learning rates are preferred that cause slower learning over a large number of training iterations. This increases the likelihood of the network finding a good set of weights across all layers rather than the fastest set of weights that minimize error (called premature convergence)."
      ],
      "id": "3PvumAAGTLHi"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Q2kFnCoVT0p"
      },
      "source": [
        "# Update network weights with error\n",
        "def update_weights(network, row, l_rate):\n",
        "\tfor i in range(len(network)):\n",
        "\t\tinputs = row[:-1]\n",
        "\t\tif i != 0:\n",
        "\t\t\tinputs = [neuron['output'] for neuron in network[i - 1]]\n",
        "\t\tfor neuron in network[i]:\n",
        "\t\t\tfor j in range(len(inputs)):\n",
        "\t\t\t\tneuron['weights'][j] += l_rate * neuron['delta'] * inputs[j]\n",
        "\t\t\tneuron['weights'][-1] += l_rate * neuron['delta']"
      ],
      "id": "1Q2kFnCoVT0p",
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z41jFmdwTVNN"
      },
      "source": [
        "## Train Network\n",
        "As mentioned, the network is updated using stochastic gradient descent.\n",
        "\n",
        "This involves first looping for a fixed number of epochs and within each epoch updating the network for each row in the training dataset.\n",
        "\n",
        "Because updates are made for each training pattern, this type of learning is called online learning. If errors were accumulated across an epoch before updating the weights, this is called batch learning or batch gradient descent.\n",
        "\n",
        "Below is a function that implements the training of an already initialized neural network with a given training dataset, learning rate, fixed number of epochs and an expected number of output values.\n",
        "\n",
        "The expected number of output values is used to transform class values in the training data into a one hot encoding. That is a binary vector with one column for each class value to match the output of the network. This is required to calculate the error for the output layer.\n",
        "\n",
        "You can also see that the sum squared error between the expected output and the network output is accumulated each epoch and printed. This is helpful to create a trace of how much the network is learning and improving each epoch."
      ],
      "id": "z41jFmdwTVNN"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "irpE-qwIXiE9"
      },
      "source": [
        "# Train a network for a fixed number of epochs\n",
        "def train_network(network, train, l_rate, n_epoch, n_outputs):\n",
        "\tfor epoch in range(n_epoch):\n",
        "\t\tsum_error = 0\n",
        "\t\tfor row in train:\n",
        "\t\t\toutputs = forward_propagate(network, row)\n",
        "\t\t\texpected = [0 for i in range(n_outputs)]\n",
        "\t\t\texpected[row[-1]] = 1\n",
        "\t\t\tsum_error += sum([(expected[i]-outputs[i])**2 for i in range(len(expected))])\n",
        "\t\t\tbackward_propagate_error(network, expected)\n",
        "\t\t\tupdate_weights(network, row, l_rate)\n",
        "\t\tprint('>epoch=%d, lrate=%.3f, error=%.3f' % (epoch, l_rate, sum_error))"
      ],
      "id": "irpE-qwIXiE9",
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IfIlKQh_XmjD"
      },
      "source": [
        "# Example\n",
        "Below is the complete example. We will use 2 neurons in the hidden layer. It is a binary classification problem (2 classes) so there will be two neurons in the output layer. The network will be trained for 20 epochs with a learning rate of 0.5, which is high because we are training for so few iterations."
      ],
      "id": "IfIlKQh_XmjD"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Seyj5BbmhPP9",
        "outputId": "444c9da4-fcff-4991-fa5a-bd43108432c4"
      },
      "source": [
        "# Test training backprop algorithm\n",
        "seed(1)\n",
        "dataset = [[2.7810836,2.550537003,0],\n",
        "\t[1.465489372,2.362125076,0],\n",
        "\t[3.396561688,4.400293529,0],\n",
        "\t[1.38807019,1.850220317,0],\n",
        "\t[3.06407232,3.005305973,0],\n",
        "\t[7.627531214,2.759262235,1],\n",
        "\t[5.332441248,2.088626775,1],\n",
        "\t[6.922596716,1.77106367,1],\n",
        "\t[8.675418651,-0.242068655,1],\n",
        "\t[7.673756466,3.508563011,1]]\n",
        "n_inputs = len(dataset[0]) - 1\n",
        "n_outputs = len(set([row[-1] for row in dataset]))\n",
        "network = initialize_network(n_inputs, 2, n_outputs)\n",
        "train_network(network, dataset, 0.5, 100, n_outputs)\n",
        "for layer in network:\n",
        "\tprint(layer)"
      ],
      "id": "Seyj5BbmhPP9",
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ">epoch=0, lrate=0.500, error=6.350\n",
            ">epoch=1, lrate=0.500, error=5.531\n",
            ">epoch=2, lrate=0.500, error=5.221\n",
            ">epoch=3, lrate=0.500, error=4.951\n",
            ">epoch=4, lrate=0.500, error=4.519\n",
            ">epoch=5, lrate=0.500, error=4.173\n",
            ">epoch=6, lrate=0.500, error=3.835\n",
            ">epoch=7, lrate=0.500, error=3.506\n",
            ">epoch=8, lrate=0.500, error=3.192\n",
            ">epoch=9, lrate=0.500, error=2.898\n",
            ">epoch=10, lrate=0.500, error=2.626\n",
            ">epoch=11, lrate=0.500, error=2.377\n",
            ">epoch=12, lrate=0.500, error=2.153\n",
            ">epoch=13, lrate=0.500, error=1.953\n",
            ">epoch=14, lrate=0.500, error=1.774\n",
            ">epoch=15, lrate=0.500, error=1.614\n",
            ">epoch=16, lrate=0.500, error=1.472\n",
            ">epoch=17, lrate=0.500, error=1.346\n",
            ">epoch=18, lrate=0.500, error=1.233\n",
            ">epoch=19, lrate=0.500, error=1.132\n",
            ">epoch=20, lrate=0.500, error=1.042\n",
            ">epoch=21, lrate=0.500, error=0.961\n",
            ">epoch=22, lrate=0.500, error=0.887\n",
            ">epoch=23, lrate=0.500, error=0.821\n",
            ">epoch=24, lrate=0.500, error=0.761\n",
            ">epoch=25, lrate=0.500, error=0.707\n",
            ">epoch=26, lrate=0.500, error=0.658\n",
            ">epoch=27, lrate=0.500, error=0.613\n",
            ">epoch=28, lrate=0.500, error=0.573\n",
            ">epoch=29, lrate=0.500, error=0.536\n",
            ">epoch=30, lrate=0.500, error=0.503\n",
            ">epoch=31, lrate=0.500, error=0.472\n",
            ">epoch=32, lrate=0.500, error=0.445\n",
            ">epoch=33, lrate=0.500, error=0.420\n",
            ">epoch=34, lrate=0.500, error=0.397\n",
            ">epoch=35, lrate=0.500, error=0.376\n",
            ">epoch=36, lrate=0.500, error=0.356\n",
            ">epoch=37, lrate=0.500, error=0.339\n",
            ">epoch=38, lrate=0.500, error=0.322\n",
            ">epoch=39, lrate=0.500, error=0.307\n",
            ">epoch=40, lrate=0.500, error=0.293\n",
            ">epoch=41, lrate=0.500, error=0.280\n",
            ">epoch=42, lrate=0.500, error=0.268\n",
            ">epoch=43, lrate=0.500, error=0.257\n",
            ">epoch=44, lrate=0.500, error=0.247\n",
            ">epoch=45, lrate=0.500, error=0.237\n",
            ">epoch=46, lrate=0.500, error=0.228\n",
            ">epoch=47, lrate=0.500, error=0.220\n",
            ">epoch=48, lrate=0.500, error=0.212\n",
            ">epoch=49, lrate=0.500, error=0.204\n",
            ">epoch=50, lrate=0.500, error=0.197\n",
            ">epoch=51, lrate=0.500, error=0.191\n",
            ">epoch=52, lrate=0.500, error=0.185\n",
            ">epoch=53, lrate=0.500, error=0.179\n",
            ">epoch=54, lrate=0.500, error=0.173\n",
            ">epoch=55, lrate=0.500, error=0.168\n",
            ">epoch=56, lrate=0.500, error=0.163\n",
            ">epoch=57, lrate=0.500, error=0.158\n",
            ">epoch=58, lrate=0.500, error=0.154\n",
            ">epoch=59, lrate=0.500, error=0.150\n",
            ">epoch=60, lrate=0.500, error=0.146\n",
            ">epoch=61, lrate=0.500, error=0.142\n",
            ">epoch=62, lrate=0.500, error=0.138\n",
            ">epoch=63, lrate=0.500, error=0.135\n",
            ">epoch=64, lrate=0.500, error=0.131\n",
            ">epoch=65, lrate=0.500, error=0.128\n",
            ">epoch=66, lrate=0.500, error=0.125\n",
            ">epoch=67, lrate=0.500, error=0.122\n",
            ">epoch=68, lrate=0.500, error=0.119\n",
            ">epoch=69, lrate=0.500, error=0.117\n",
            ">epoch=70, lrate=0.500, error=0.114\n",
            ">epoch=71, lrate=0.500, error=0.112\n",
            ">epoch=72, lrate=0.500, error=0.109\n",
            ">epoch=73, lrate=0.500, error=0.107\n",
            ">epoch=74, lrate=0.500, error=0.105\n",
            ">epoch=75, lrate=0.500, error=0.103\n",
            ">epoch=76, lrate=0.500, error=0.101\n",
            ">epoch=77, lrate=0.500, error=0.099\n",
            ">epoch=78, lrate=0.500, error=0.097\n",
            ">epoch=79, lrate=0.500, error=0.095\n",
            ">epoch=80, lrate=0.500, error=0.093\n",
            ">epoch=81, lrate=0.500, error=0.092\n",
            ">epoch=82, lrate=0.500, error=0.090\n",
            ">epoch=83, lrate=0.500, error=0.088\n",
            ">epoch=84, lrate=0.500, error=0.087\n",
            ">epoch=85, lrate=0.500, error=0.085\n",
            ">epoch=86, lrate=0.500, error=0.084\n",
            ">epoch=87, lrate=0.500, error=0.083\n",
            ">epoch=88, lrate=0.500, error=0.081\n",
            ">epoch=89, lrate=0.500, error=0.080\n",
            ">epoch=90, lrate=0.500, error=0.079\n",
            ">epoch=91, lrate=0.500, error=0.077\n",
            ">epoch=92, lrate=0.500, error=0.076\n",
            ">epoch=93, lrate=0.500, error=0.075\n",
            ">epoch=94, lrate=0.500, error=0.074\n",
            ">epoch=95, lrate=0.500, error=0.073\n",
            ">epoch=96, lrate=0.500, error=0.072\n",
            ">epoch=97, lrate=0.500, error=0.071\n",
            ">epoch=98, lrate=0.500, error=0.070\n",
            ">epoch=99, lrate=0.500, error=0.069\n",
            "[{'weights': [-1.8904818823786256, 2.6029577588462405, 1.36922598873631], 'output': 0.018283261957935986, 'delta': -0.0006070497524612589}, {'weights': [0.9538894079909441, -1.386542269801398, -0.19848619686044763], 'output': 0.9010494694007615, 'delta': 0.001321884937561888}]\n",
            "[{'weights': [4.447262579901695, -1.624718015475665, -1.26932683693348], 'output': 0.0660956654307449, 'delta': -0.004079889019937295}, {'weights': [-4.158762896273457, 2.1783932549616014, 0.808710771335537], 'output': 0.9365588447655114, 'delta': 0.0037694434734618643}]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i-oFEF1uTRf8"
      },
      "source": [
        "# Predicting\n",
        "In this case we will take the class with highest probaility as the final prediction"
      ],
      "id": "i-oFEF1uTRf8"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F90zeKRiYCCn"
      },
      "source": [
        "# Make a prediction with a network\n",
        "def predict(network, row):\n",
        "\toutputs = forward_propagate(network, row)\n",
        "\treturn outputs.index(max(outputs))"
      ],
      "id": "F90zeKRiYCCn",
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P1StNA4rYaZp",
        "outputId": "c9b28ba1-d834-4dc6-9356-0a3251f08098",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Testing predictions\n",
        "# Test making predictions with the network\n",
        "dataset = [[2.7810836,2.550537003,0],\n",
        "\t[1.465489372,2.362125076,0],\n",
        "\t[3.396561688,4.400293529,0],\n",
        "\t[1.38807019,1.850220317,0],\n",
        "\t[3.06407232,3.005305973,0],\n",
        "\t[7.627531214,2.759262235,1],\n",
        "\t[5.332441248,2.088626775,1],\n",
        "\t[6.922596716,1.77106367,1],\n",
        "\t[8.675418651,-0.242068655,1],\n",
        "\t[7.673756466,3.508563011,1]]\n",
        "network = [[{'weights': [-1.482313569067226, 1.8308790073202204, 1.078381922048799]}, {'weights': [0.23244990332399884, 0.3621998343835864, 0.40289821191094327]}],\n",
        "\t[{'weights': [2.5001872433501404, 0.7887233511355132, -1.1026649757805829]}, {'weights': [-2.429350576245497, 0.8357651039198697, 1.0699217181280656]}]]\n",
        "for row in dataset:\n",
        "\tprediction = predict(network, row)\n",
        "\tprint('Expected=%d, Got=%d' % (row[-1], prediction))"
      ],
      "id": "P1StNA4rYaZp",
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Expected=0, Got=0\n",
            "Expected=0, Got=0\n",
            "Expected=0, Got=0\n",
            "Expected=0, Got=0\n",
            "Expected=0, Got=0\n",
            "Expected=1, Got=1\n",
            "Expected=1, Got=1\n",
            "Expected=1, Got=1\n",
            "Expected=1, Got=1\n",
            "Expected=1, Got=1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zzn99LSMYxnp"
      },
      "source": [
        "# Using Wheat seeds dataset"
      ],
      "id": "Zzn99LSMYxnp"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jbH-OypuY2i5",
        "outputId": "35af6b39-7578-4e3e-e892-b7277156f3ee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        }
      },
      "source": [
        "# Backprop on the Seeds Dataset\n",
        "from random import seed\n",
        "from random import randrange\n",
        "from random import random\n",
        "from csv import reader\n",
        "from math import exp\n",
        " \n",
        "# Load a CSV file\n",
        "def load_csv(filename):\n",
        "\tdataset = list()\n",
        "\twith open(filename, 'r') as file:\n",
        "\t\tcsv_reader = reader(file)\n",
        "\t\tfor row in csv_reader:\n",
        "\t\t\tif not row:\n",
        "\t\t\t\tcontinue\n",
        "\t\t\tdataset.append(row)\n",
        "\treturn dataset\n",
        " \n",
        "# Convert string column to float\n",
        "def str_column_to_float(dataset, column):\n",
        "\tfor row in dataset:\n",
        "\t\trow[column] = float(row[column].strip())\n",
        " \n",
        "# Convert string column to integer\n",
        "def str_column_to_int(dataset, column):\n",
        "\tclass_values = [row[column] for row in dataset]\n",
        "\tunique = set(class_values)\n",
        "\tlookup = dict()\n",
        "\tfor i, value in enumerate(unique):\n",
        "\t\tlookup[value] = i\n",
        "\tfor row in dataset:\n",
        "\t\trow[column] = lookup[row[column]]\n",
        "\treturn lookup\n",
        " \n",
        "# Find the min and max values for each column\n",
        "def dataset_minmax(dataset):\n",
        "\tminmax = list()\n",
        "\tstats = [[min(column), max(column)] for column in zip(*dataset)]\n",
        "\treturn stats\n",
        " \n",
        "# Rescale dataset columns to the range 0-1\n",
        "def normalize_dataset(dataset, minmax):\n",
        "\tfor row in dataset:\n",
        "\t\tfor i in range(len(row)-1):\n",
        "\t\t\trow[i] = (row[i] - minmax[i][0]) / (minmax[i][1] - minmax[i][0])\n",
        " \n",
        "# Split a dataset into k folds\n",
        "def cross_validation_split(dataset, n_folds):\n",
        "\tdataset_split = list()\n",
        "\tdataset_copy = list(dataset)\n",
        "\tfold_size = int(len(dataset) / n_folds)\n",
        "\tfor i in range(n_folds):\n",
        "\t\tfold = list()\n",
        "\t\twhile len(fold) < fold_size:\n",
        "\t\t\tindex = randrange(len(dataset_copy))\n",
        "\t\t\tfold.append(dataset_copy.pop(index))\n",
        "\t\tdataset_split.append(fold)\n",
        "\treturn dataset_split\n",
        " \n",
        "# Calculate accuracy percentage\n",
        "def accuracy_metric(actual, predicted):\n",
        "\tcorrect = 0\n",
        "\tfor i in range(len(actual)):\n",
        "\t\tif actual[i] == predicted[i]:\n",
        "\t\t\tcorrect += 1\n",
        "\treturn correct / float(len(actual)) * 100.0\n",
        " \n",
        "# Evaluate an algorithm using a cross validation split\n",
        "def evaluate_algorithm(dataset, algorithm, n_folds, *args):\n",
        "\tfolds = cross_validation_split(dataset, n_folds)\n",
        "\tscores = list()\n",
        "\tfor fold in folds:\n",
        "\t\ttrain_set = list(folds)\n",
        "\t\ttrain_set.remove(fold)\n",
        "\t\ttrain_set = sum(train_set, [])\n",
        "\t\ttest_set = list()\n",
        "\t\tfor row in fold:\n",
        "\t\t\trow_copy = list(row)\n",
        "\t\t\ttest_set.append(row_copy)\n",
        "\t\t\trow_copy[-1] = None\n",
        "\t\tpredicted = algorithm(train_set, test_set, *args)\n",
        "\t\tactual = [row[-1] for row in fold]\n",
        "\t\taccuracy = accuracy_metric(actual, predicted)\n",
        "\t\tscores.append(accuracy)\n",
        "\treturn scores\n",
        " \n",
        "\n",
        " \n",
        "# Test Backprop on Seeds dataset\n",
        "seed(1)\n",
        "# load and prepare data\n",
        "filename = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/wheat-seeds.csv'\n",
        "dataset = load_csv(filename)\n",
        "for i in range(len(dataset[0])-1):\n",
        "\tstr_column_to_float(dataset, i)\n",
        "# convert class column to integers\n",
        "str_column_to_int(dataset, len(dataset[0])-1)\n",
        "# normalize input variables\n",
        "minmax = dataset_minmax(dataset)\n",
        "normalize_dataset(dataset, minmax)\n",
        "# evaluate algorithm\n",
        "n_folds = 5\n",
        "l_rate = 0.3\n",
        "n_epoch = 500\n",
        "n_hidden = 5\n",
        "scores = evaluate_algorithm(dataset, back_propagation, n_folds, l_rate, n_epoch, n_hidden)\n",
        "print('Scores: %s' % scores)\n",
        "print('Mean Accuracy: %.3f%%' % (sum(scores)/float(len(scores))))"
      ],
      "id": "jbH-OypuY2i5",
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-6c6a4a6ba35b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;31m# load and prepare data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'https://raw.githubusercontent.com/jbrownlee/Datasets/master/wheat-seeds.csv'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0mstr_column_to_float\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-26-6c6a4a6ba35b>\u001b[0m in \u001b[0;36mload_csv\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m                 \u001b[0mcsv_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcsv_reader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/wheat-seeds.csv'"
          ]
        }
      ]
    }
  ]
}