{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(X, y, time_steps_x, time_steps_y):\n",
    "    \"\"\"\n",
    "    Creating windowed dataset for feeding into the neural network\n",
    "\n",
    "    Arguments :\n",
    "\n",
    "    df : dataframe which has to be converted to 3D array from 2D array\n",
    "    window_size : timeperiod during which the model must be trained on a rolling basis\n",
    "\n",
    "    Returns:\n",
    "\n",
    "    np.array(arr) : an array of arrays where every array is a window (3D array)\n",
    "\n",
    "    \"\"\"\n",
    "    Xs, ys = [], []\n",
    "    for i in range(len(X) - time_steps_x - time_steps_y +1):\n",
    "        Xs.append(X.iloc[i:(i + time_steps_x)].values)\n",
    "        \n",
    "    for i in range(time_steps_x, len(X) - time_steps_y + 1):\n",
    "        ys.append(y.iloc[i:i + time_steps_y].values)\n",
    "    return np.array(Xs), np.array(ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the neural network structure\n",
    "\n",
    "# Kernel Initialiser is used to initialise the random weights in the neural net. Here we are using glorot.\n",
    "# in the initializer for kernels we can put in a seed that will prevent the random initialization.\n",
    "# https://keras.io/api/layers/initializers/\n",
    "\n",
    "#return_sequences transfers the hidden state from one cell to another\n",
    "#return_Sequences are required for stacking LSTMS so that they provide the hidden state for each time step \n",
    "# rather than combining it\n",
    "\n",
    "# kernel regulizers are used\n",
    "\n",
    "# Relu activation is used\n",
    "\n",
    "# Activation functions\n",
    "# https://machinelearningmastery.com/choose-an-activation-function-for-deep-learning/#:~:text=Activation%20functions%20are%20a%20critical,design%20of%20a%20neural%20network.&text=Activation%20functions%20are%20a%20key,the%20type%20of%20prediction%20problem.\n",
    "\n",
    "def autoencoder_model(array):\n",
    "    \"\"\"\n",
    "    Defining the neural network structure\n",
    "\n",
    "    Arguments:\n",
    "\n",
    "    array : array of predictor variable (gauge column)\n",
    "\n",
    "    Returns:\n",
    "\n",
    "    model : model to be used is returned\n",
    "\n",
    "    \"\"\"\n",
    "    inputs = Input(shape = (array.shape[1], array.shape[2]))  \n",
    "    L1 = LSTM(128, kernel_initializer=initializers.glorot_normal(seed_no), activation='relu', return_sequences=True, kernel_regularizer= regularizers.l2(reg_lambda))(inputs)\n",
    "    L2 = LSTM(64, kernel_initializer=initializers.glorot_normal(seed_no), activation='relu', return_sequences=True)(L1)\n",
    "    L3 = LSTM(32, kernel_initializer=initializers.glorot_normal(seed_no), activation='relu', return_sequences=False)(L2)\n",
    "    L4 = RepeatVector(array.shape[1])(L3)\n",
    "    L5 = LSTM(32, kernel_initializer=initializers.glorot_normal(seed_no), activation='relu', return_sequences=True)(L4)\n",
    "    L6 = LSTM(64, kernel_initializer=initializers.glorot_normal(seed_no), activation='relu', return_sequences=True)(L5)\n",
    "    L7 = LSTM(128, kernel_initializer=initializers.glorot_normal(seed_no), activation='relu', return_sequences=True)(L6)\n",
    "  \n",
    "    output = TimeDistributed(Dense(array.shape[2], kernel_initializer=initializers.glorot_normal(seed_no)))(L7)\n",
    "    model = Model(inputs = inputs, outputs = output)\n",
    "    return model\n",
    "\n",
    "\n",
    "def hvd_train(train_list, learning_rate,reg_lambda, batch_size, epochs):\n",
    "    \"\"\"\n",
    "    Training the wells\n",
    "\n",
    "    Arguments :\n",
    "\n",
    "    train_list : list of training wells\n",
    "    learning_rate : learning rate of the model (hyperparameter)\n",
    "    reg_lambda : regularisation parameter of the model (hyperparameter)\n",
    "    batch_size : training samples of the model (hyperparameter)\n",
    "    epochs : number of times the algorithm will work through the data set (hyperparameter)\n",
    "\n",
    "    Returns:\n",
    "\n",
    "    AE_model : auto encoder model to be used\n",
    "    timestamp_dict : maximum timestamp for which each well is predicted\n",
    "    History : model fit object (used to obtain validation loss and loss)\n",
    "\n",
    "    \"\"\"\n",
    "    # Horovod: pin GPU to be used to process local rank (one GPU per process)\n",
    "    global History\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "    config.gpu_options.visible_device_list = str(hvd.local_rank())\n",
    "    K.set_session(tf.Session(config=config))\n",
    "\n",
    "    timestamp_dict = {}\n",
    "    # train_csv has been cleaned of anomalies based on LPO codes\n",
    "    train_csv = read_csv('filename.csv',data_path) \n",
    "    max_timestamp = str(train_csv.TimeStamp.max())\n",
    "    timestamp_dict[train_list[0]] = max_timestamp\n",
    "\n",
    "    ### Data pre-processing for the training well\n",
    "    # Considering only gauge columns\n",
    "    train = train_csv[gauge_cols]\n",
    "\n",
    "    # Normalizing data\n",
    "    scaler = MinMaxScaler()\n",
    "    X_train = scaler.fit_transform(train)\n",
    "    X_train = create_dataset(X_train, window_size)\n",
    "\n",
    "    # Creating test dataset by taking a chunk of the timeseries which is 20%\n",
    "    x_test = X_train[math.floor(0.4*len(X_train)):math.floor(0.6*len(X_train))]\n",
    "    x_train_a = X_train[:math.floor(0.4*len(X_train))]\n",
    "    x_train_b = X_train[math.floor(0.6*len(X_train)):]\n",
    "    x_train = np.concatenate((x_train_a, x_train_b), axis=0, out=None)\n",
    "\n",
    "    AE_model = autoencoder_model(x_train)\n",
    "\n",
    "    # Adjusting learning rate based on number of GPUs.\n",
    "    optimizer = keras.optimizers.SGD(lr = learning_rate*hvd.size())\n",
    "\n",
    "    # Wrapping our optimizer with Horovod's distributed optimizer\n",
    "    optimizer = hvd.DistributedOptimizer(optimizer)\n",
    "\n",
    "    AE_model.compile(optimizer = optimizer, loss='mae')\n",
    "\n",
    "    # Broadcasting initial variable states from rank 0 to all other processes.\n",
    "    # This is necessary to ensure consistent initialization of all workers when\n",
    "    # training is started with random weights or restored from a checkpoint.\n",
    "    callbacks = [hvd.callbacks.BroadcastGlobalVariablesCallback(0),]\n",
    "\n",
    "    # Saving checkpoints only on worker 0 to prevent other workers from corrupting them.\n",
    "    if hvd.rank() == 0:\n",
    "        callbacks.append(keras.callbacks.ModelCheckpoint('./checkpoint-{epoch}.h5'))\n",
    "\n",
    "    History = AE_model.fit(x_train, x_train,\n",
    "            batch_size = batch_size,\n",
    "            callbacks = callbacks,\n",
    "            epochs = epochs,\n",
    "            verbose = 2,\n",
    "            validation_data = (x_test, x_test))\n",
    "    return AE_model,timestamp_dict, History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def indices(indices):\n",
    "  \"\"\"\n",
    "  Getting start and end index from a range of indices\n",
    "  \n",
    "  \"\"\"\n",
    "  from itertools import groupby, count\n",
    "  def as_range(iterable):\n",
    "    l = list(iterable)\n",
    "    if len(l) > 1:\n",
    "      return '{0}-{1}'.format(l[0], l[-1])\n",
    "    else:\n",
    "      return '{0}'.format(l[0])\n",
    "  # Gathering BU indices\n",
    "  indices = ','.join(as_range(g) for _, g in groupby(indices, key=lambda n, c=count(): n-next(c)))\n",
    "  indices = indices.split(\",\")\n",
    "  indices = [i.split(\"-\") for i in indices]\n",
    "  bu_list = []\n",
    "  for i in range(len(indices)):\n",
    "    if len(indices[i]) == 2:\n",
    "      bu_list.append(indices[i])\n",
    "  indices = []\n",
    "  for sublist in bu_list:\n",
    "      for item in sublist:\n",
    "          indices.append(item)\n",
    "  return [int(i) for i in indices]\n",
    "\n",
    "def plots_with_ranges():\n",
    "  for j in columns:\n",
    "    fig, ax1 = plt.subplots(figsize = (18, 5))\n",
    "\n",
    "    color = 'tab:red'\n",
    "    ax1.set_xlabel('Time (10 Minutes)')\n",
    "    ax1.set_ylabel(j, color=color)\n",
    "    ax1.plot(concat_data.index, concat_data[j] , 'blue', label = 'non-anomalous pred')\n",
    "    ax1.scatter(anomaly.index, anomaly[j] , color='red', s=10, label='anomalous pred')\n",
    "    ax1.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "    ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\n",
    "\n",
    "    color = 'tab:blue'\n",
    "    ax2.set_ylabel('risk_factor', color=color)  # we already handled the x-label with ax1\n",
    "    ax2.plot(risk_factor.index, risk_factor, color='black')\n",
    "    ax2.tick_params(axis='y', labelcolor=color)\n",
    "    plt.title(str(j)+' vs Time')\n",
    "\n",
    "    for i in range(int(len(bu_index)/2)):\n",
    "      i = 2 * i\n",
    "      plt.axvspan(bu_index[i], bu_index[i+1], color='green', alpha=0.2, label='buildup')\n",
    "      plt.axvspan(bu_index[i+1] - 144, bu_index[i+1], color='red', alpha=0.2, label='buildup')\n",
    "      \n",
    "    plt.savefig('fig.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 3, 5, 7, 10, 11, 15, 16]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices([1,2,3,5,6,7,10,11,13,15,16])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:625: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "C:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:19: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# import tensorflow as tf\n",
    "# from tensorflow import keras\n",
    "import pandas as pd\n",
    "# import seaborn as sns\n",
    "# from pylab import rcParams\n",
    "# import matplotlib.pyplot as plt\n",
    "# from matplotlib import rc\n",
    "# from pandas.plotting import register_matplotlib_converters\n",
    "\n",
    "\n",
    "\n",
    "# importing \n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from numpy.random import seed\n",
    "seed(seed_no)\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(seed_no)\n",
    "import random\n",
    "random.seed(seed_no)\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools as it\n",
    "from datetime import date,datetime\n",
    "from time import time\n",
    "\n",
    "# Keras libs\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dropout, Dense, Flatten, Input, LSTM, TimeDistributed, RepeatVector\n",
    "from keras import backend as K\n",
    "from keras.models import Model\n",
    "from keras import regularizers\n",
    "from keras import initializers\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import tensorflow as tf\n",
    "import horovod.keras as hvd\n",
    "\n",
    "import datetime\n",
    "\n",
    "hvd.init()\n",
    "\n",
    "a = [[1,2,3], [4,5,6], [7,8,9], [10,11,12]]\n",
    "df = pd.DataFrame(a, columns = ['A', 'B', 'C']) \n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler = scaler.fit(df[['A', 'B', 'C']])\n",
    "\n",
    "sc = scaler.transform(df[['A', 'B', 'C']])\n",
    "df_scaled = pd.DataFrame(sc, columns = ['A', 'B', 'C']) \n",
    "\n",
    "\n",
    "\n",
    "time_steps = 2\n",
    "\n",
    "# reshape to [samples, time_steps, n_features]\n",
    "\n",
    "X_train, y_train = create_dataset(\n",
    "      df_scaled[['A', 'B']],\n",
    "      df_scaled['C'],\n",
    "      1,3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
